{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as tfk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers as tfkl\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Keras version is {tfk.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_PATH = Path().absolute().parent / \"data\" / \"training_set.npz\"\n",
    "DATA_PATH = LOCAL_DATA_PATH\n",
    "OUTPUT_DIR = Path().absolute().parent / \"submission\"\n",
    "# If the data is not there, then we're running in Kaggle\n",
    "if not LOCAL_DATA_PATH.exists():\n",
    "    print(\"Running on Kaggle\")\n",
    "    DATA_PATH = Path(\"/kaggle/input/an2dl-homework-1/training_set.npz\")\n",
    "    OUTPUT_DIR = Path()\n",
    "\n",
    "print(DATA_PATH)\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "METRICS = [\"accuracy\", \"recall\", \"f1_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfk.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    test_size = 1500\n",
    "    # Augmentation\n",
    "    augmentation_fill = \"wrap\"\n",
    "    translation_factor = (-.15, .15)\n",
    "    rotation_factor = 1.0\n",
    "    # Model\n",
    "    layer_units = (256, 64)\n",
    "    activation = \"silu\"\n",
    "    # Training\n",
    "    optimiser = tfk.optimizers.AdamW\n",
    "    learning_rate = 1e-4\n",
    "    loss = \"categorical_crossentropy\"\n",
    "    epochs = 500\n",
    "    ## Early stopping parameters\n",
    "    es_patience = 20\n",
    "    es_min_delta = 1e-5\n",
    "    ## Learning rate schedule\n",
    "    lr_patience = 10\n",
    "    lr_decay_factor = 0.1\n",
    "    min_lr = 1e-7\n",
    "\n",
    "hp = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWidth(Enum):\n",
    "    EfficientNetV2B3 = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(DATA_PATH) as data:\n",
    "    images = data[\"images\"]\n",
    "    labels = tfk.utils.to_categorical(data[\"labels\"])\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove polluted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_means: np.ndarray = images.mean(axis=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOLERANCE = 1e-4\n",
    "MIN_DUPLICATES = 5\n",
    "\n",
    "mean_indices = {}\n",
    "for mean in image_means:\n",
    "    indices = (np.abs(mean - image_means) <= TOLERANCE).nonzero()[0]\n",
    "    if len(indices) > MIN_DUPLICATES:\n",
    "        mean_indices[mean] = indices\n",
    "\n",
    "len(mean_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_indices = []\n",
    "for indices in mean_indices.values():\n",
    "    duplicated_indices.extend(indices)\n",
    "duplicated_indices = np.array(duplicated_indices)\n",
    "duplicated_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_indices = np.setdiff1d(\n",
    "    list(range(images.shape[0])),\n",
    "    duplicated_indices\n",
    ")\n",
    "original_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_images = images[original_indices]\n",
    "clean_labels = labels[original_indices]\n",
    "\n",
    "print(clean_images.shape)\n",
    "print(clean_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    clean_images, clean_labels,\n",
    "    test_size=hp.test_size,\n",
    "    random_state=SEED,\n",
    "    stratify=clean_labels,\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(hp: Hyperparameters) -> tfk.Model:\n",
    "    inputs = tfkl.Input((96, 96, 3))\n",
    "    x = inputs\n",
    "\n",
    "    # Augmentation\n",
    "    x = tfkl.RandomTranslation(\n",
    "        hp.translation_factor,  # Height\n",
    "        hp.translation_factor,  # Width\n",
    "        fill_mode=hp.augmentation_fill,\n",
    "        seed=SEED,\n",
    "    )(x)\n",
    "    x = tfkl.RandomRotation(\n",
    "        hp.rotation_factor,\n",
    "        fill_mode=hp.augmentation_fill,\n",
    "        seed=SEED,\n",
    "    )(x)\n",
    "\n",
    "    # ImageNet images are 224x224 so we need to resize to use the\n",
    "    # pre-trained backbone feature extractor\n",
    "    x = tfkl.Resizing(224, 224)(x)\n",
    "\n",
    "    # TODO Move feature extractor to hyperparameters\n",
    "    # TODO Investigate changing `input_shape` to actual image shape\n",
    "    # instead of resizing\n",
    "    feature_extractor = tfk.applications.MobileNetV3Large(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling=\"max\",\n",
    "    )\n",
    "    feature_extractor.trainable = False\n",
    "    x = feature_extractor(x)\n",
    "\n",
    "    for units in Hyperparameters.layer_units:\n",
    "        x = tfkl.Dense(\n",
    "            units,\n",
    "            activation=hp.activation,\n",
    "        )(x)\n",
    "\n",
    "    # Classification problem has 8 output classes\n",
    "    # so the final layer has 8 neurons\n",
    "    # with a softmax activation\n",
    "    outputs = tfkl.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "    return tfk.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build(hp)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(hp: Hyperparameters, train: tf.data.Dataset, val: tf.data.Dataset) -> tuple[tfk.Model, tfk.callbacks.History]:\n",
    "    model = build(hp)\n",
    "    model.compile(\n",
    "        optimizer=hp.optimiser(hp.learning_rate),\n",
    "        loss=hp.loss,\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train.prefetch(tf.data.AUTOTUNE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=hp.epochs,\n",
    "        validation_data=val.prefetch(tf.data.AUTOTUNE),\n",
    "        callbacks=[\n",
    "            tfk.callbacks.EarlyStopping(\n",
    "                min_delta=hp.es_min_delta,\n",
    "                patience=hp.es_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1,\n",
    "            ),\n",
    "            tfk.callbacks.ReduceLROnPlateau(\n",
    "                factor=hp.lr_decay_factor,\n",
    "                patience=hp.lr_patience,\n",
    "                min_lr=hp.min_lr,\n",
    "                verbose=1,\n",
    "            )\n",
    "        ],\n",
    "        verbose=2,\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = fit(hp, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
