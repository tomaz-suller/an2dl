{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:11:41.624499: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-16 19:11:41.625341: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-16 19:11:41.667919: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-16 19:11:41.824181: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-16 19:11:42.649675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import ClassVar\n",
    "\n",
    "import keras as tfk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers as tfkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Keras version is {tfk.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTED_DIR = Path().absolute().parent / \"data\" / \"augmented\"\n",
    "OUTPUT_DIR = Path().absolute().parent / \"submission\"\n",
    "# If the data is not there, then we're running in Kaggle\n",
    "if not AUGMENTED_DIR.exists():\n",
    "    AUGMENTED_DIR = Path(\"/kaggle/input/an2dl-homework-1-augmented/augmented\")\n",
    "    OUTPUT_DIR = Path()\n",
    "\n",
    "TRAIN_TFDS_DIR = AUGMENTED_DIR / \"train\"\n",
    "VAL_TFDS_DIR = AUGMENTED_DIR / \"val\"\n",
    "\n",
    "print(TRAIN_TFDS_DIR)\n",
    "print(VAL_TFDS_DIR)\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "KERAS_VERBOSITY = 2\n",
    "BATCH_SIZE = 128\n",
    "LOSS = \"categorical_crossentropy\"\n",
    "METRICS = [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfk.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    # Model\n",
    "    global_pooling_type: str = \"avg\"\n",
    "    layer_units: tuple[int, ...] = (\n",
    "        128,\n",
    "        64,\n",
    "    )\n",
    "    activation: str = \"silu\"\n",
    "    class_weight: ClassVar[dict[int, float]] = {\n",
    "        0: 1.00,\n",
    "        1: 1.00,\n",
    "        2: 1.00,\n",
    "        3: 1.00,\n",
    "        4: 1.00,\n",
    "        5: 1.00,\n",
    "        6: 1.00,\n",
    "        7: 1.00,\n",
    "    }\n",
    "    ## Regularisation\n",
    "    regularisations: tuple[tuple[float, float]] = (\n",
    "        (1e-3, 0.0),\n",
    "        (0.0, 0.0),\n",
    "    )\n",
    "    dropout: float = 0.6\n",
    "    noise_std: float = 0.075\n",
    "    # Training\n",
    "    optimiser: tfk.Optimizer = tfk.optimizers.AdamW\n",
    "    learning_rate: float = 1e-4\n",
    "    epochs: int = 50\n",
    "    unfreeze_last: int = 70  # For fine-tuning\n",
    "    ## Early stopping parameters\n",
    "    es_patience: int = 5\n",
    "    es_min_delta: float = 1e-3\n",
    "    ## Learning rate schedule\n",
    "    lr_patience: int = 20\n",
    "    lr_min_delta: float = 1e-3\n",
    "    lr_decay_factor: float = 0.1\n",
    "    min_lr: float = 1e-8\n",
    "\n",
    "hp = Hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.load(str(TRAIN_TFDS_DIR)).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.load(str(VAL_TFDS_DIR)).batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(hp: Hyperparameters) -> tuple[tfk.Model, tfk.Model]:\n",
    "    inputs = tfkl.Input((96, 96, 3))\n",
    "    x = inputs\n",
    "\n",
    "    x = tfkl.Rescaling(1.0/255.0)(x)\n",
    "    x = tfkl.GaussianNoise(hp.noise_std)(x)\n",
    "    x = tfkl.Rescaling(255.0)(x)\n",
    "\n",
    "    # TODO Investigate changing `input_shape` to actual image shape\n",
    "    # instead of resizing\n",
    "    feature_extractor = tfk.applications.EfficientNetV2B3(\n",
    "        input_shape=(96, 96, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling=hp.global_pooling_type,\n",
    "    )\n",
    "    feature_extractor.trainable = False\n",
    "    x = feature_extractor(x)\n",
    "\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "\n",
    "    for units, regularisation in zip(hp.layer_units, hp.regularisations):\n",
    "        x = tfkl.Dense(\n",
    "            units,\n",
    "            activation=hp.activation,\n",
    "            kernel_regularizer=tfk.regularizers.L1L2(*regularisation),\n",
    "        )(x)\n",
    "\n",
    "    x = tfkl.Dropout(hp.dropout)(x)\n",
    "\n",
    "    # Classification problem has 8 output classes\n",
    "    # so the final layer has 8 neurons\n",
    "    # with a softmax activation\n",
    "    outputs = tfkl.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tfk.Model(inputs, outputs)\n",
    "\n",
    "    return feature_extractor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor, model = build(hp)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    hp: Hyperparameters,\n",
    "    train: tf.data.Dataset,\n",
    "    val: tf.data.Dataset,\n",
    "    model: tfk.Model | None = None,\n",
    ") -> tuple[tfk.Model, tfk.callbacks.History]:\n",
    "    pprint(asdict(hp))\n",
    "\n",
    "    if model is None:\n",
    "        _, model = build(hp)\n",
    "    model.compile(\n",
    "        optimizer=hp.optimiser(hp.learning_rate),\n",
    "        loss=LOSS,\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train.prefetch(tf.data.AUTOTUNE),\n",
    "        epochs=hp.epochs,\n",
    "        validation_data=val.prefetch(tf.data.AUTOTUNE),\n",
    "        class_weight=hp.class_weight,\n",
    "        callbacks=[\n",
    "            tfk.callbacks.EarlyStopping(\n",
    "                min_delta=hp.es_min_delta,\n",
    "                patience=hp.es_patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1,\n",
    "            ),\n",
    "            tfk.callbacks.ReduceLROnPlateau(\n",
    "                factor=hp.lr_decay_factor,\n",
    "                patience=hp.lr_patience,\n",
    "                min_delta=hp.lr_min_delta,\n",
    "                min_lr=hp.min_lr,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ],\n",
    "        verbose=KERAS_VERBOSITY,\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = fit(hp, train_dataset, val_dataset)\n",
    "histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in feature_extractor.layers[-hp.unfreeze_last:]:\n",
    "    if not isinstance(layer, tfkl.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "hp.epochs = 200\n",
    "hp.learning_rate = 1e-5\n",
    "hp.es_patience = 30\n",
    "hp.lr_patience = 10\n",
    "\n",
    "model, history = fit(hp, train_dataset, val_dataset, model=model)\n",
    "histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(OUTPUT_DIR / \"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (OUTPUT_DIR / \"histories.pkl\").open(\"wb\") as f:\n",
    "    pickle.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    OUTPUT_DIR / \"train_predictions\",\n",
    "    model.predict(train_dataset, verbose=KERAS_VERBOSITY),\n",
    ")\n",
    "np.save(\n",
    "    OUTPUT_DIR / \"val_predictions\",\n",
    "    model.predict(val_dataset, verbose=KERAS_VERBOSITY),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
